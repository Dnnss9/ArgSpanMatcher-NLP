{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ⚪ **Approche Naïve : Baseline Lexicales et Fréquentielle**\n",
        "\n",
        "Avant d'utiliser des architectures complexes de type Transformers, nous mettons en place une approche naïve. Cette étape est fondamentale pour quantifier la difficulté de la tâche et justifier l'utilisation d'approche plus complexe.\n",
        "\n",
        "Cette étape permet également de mesurer l'impact du Domain Shift. En appliquant un dictionnaire appris sur des essais étudiants à des résumés médicaux, nous quantifions les limites de la reconnaissance de mots-clés face à une terminologie technique inconnue. Cette baseline justifie ainsi l'utilité des modèles profonds comme RoBERTa pour capturer la sémantique là où les statistiques de mots échouent."
      ],
      "metadata": {
        "id": "ujKc-_xRna3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "##**Préparation des Données et Apprentissage Lexicale**\n",
        "\n",
        "Dans cette première étape, nous procédons au chargement des données et à la construction de notre modèle de référence. Cette approche est qualifiée de naïve car elle repose exclusivement sur la fréquence statistique des mots, faisant abstraction du contexte global de la phrase ou de la structure logique du discours.\n",
        "\n",
        "Le processus débute par un chargement multi-domaine via la fonction `load_and_extract` afin de récupérer les textes et les étiquettes BIO pour le domaine source des essais et le domaine cible médical. Ensuite, nous effectuons une analyse statistique sur l'ensemble d'entraînement pour comptabiliser, pour chaque mot unique, le label qui lui est le plus souvent attribué. Enfin, nous stabilisons cette règle de décision dans un dictionnaire nommé `naive_model` qui servira de base unique pour toutes nos prédictions. Cette méthode permet de tester l'hypothèse selon laquelle certains mots seraient des indicateurs argumentatifs suffisants, indépendamment de leur position ou de la sémantique complexe de la phrase."
      ],
      "metadata": {
        "id": "hU7RYpMwojAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "data_dir = Path(\"data\")\n",
        "\n",
        "def load_and_extract(file_name):\n",
        "    \"\"\"\n",
        "    Charge un fichier JSON et extrait les séquences de mots et leurs étiquettes BIO.\n",
        "\n",
        "    Args:\n",
        "        file_name (str): Nom du fichier à charger dans le dossier data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sentences, labels) où chaque élément est une liste de listes.\n",
        "    \"\"\"\n",
        "    path = data_dir / file_name\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    sentences, labels = [], []\n",
        "    for doc in data:\n",
        "        for paragraph in doc[\"tokens\"]:\n",
        "            if not paragraph: continue\n",
        "            words = [token[\"str\"] for token in paragraph]\n",
        "            tags = [token[\"arg\"] for token in paragraph]\n",
        "            sentences.append(words)\n",
        "            labels.append(tags)\n",
        "    return sentences, labels\n",
        "\n",
        "train_texts, train_tags = load_and_extract(\"aae_train.json\")\n",
        "test_essays_texts, test_essays_tags = load_and_extract(\"aae_test.json\")\n",
        "test_med_texts, test_med_tags = load_and_extract(\"abstrct_neoplasm_test.json\")\n",
        "\n",
        "# Création d'un dictionnaire qui compte la fréquence des labels pour chaque mot\n",
        "word_to_tag = {}\n",
        "for words, tags in zip(train_texts, train_tags):\n",
        "    for w, t in zip(words, tags):\n",
        "        w = w.lower()\n",
        "        if w not in word_to_tag:\n",
        "            word_to_tag[w] = Counter()\n",
        "        word_to_tag[w][t] += 1\n",
        "\n",
        "naive_model = {w: c.most_common(1)[0][0] for w, c in word_to_tag.items()}\n"
      ],
      "metadata": {
        "id": "7Z2Qh4EkokeO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "##**Prédictions**\n",
        "Dans cette cellule on va transformer les séquence de mot en prédiction d'arguments. Sa particularité est d'intégrer une règle de lissage structurel pour garantir que les résultats respectent les contraintes du format BIO.\n",
        "\n",
        "Le modèle parcourt la phrase et attribue à chaque mot le label le plus fréquent trouvé lors de l'entraînement. Pour les mots absents du dictionnaire, l'étiquette par défaut est `O`.\n",
        "\n",
        "Pour éviter les prédictions invalides (comme un label de continuation `I-` qui apparaîtrait seul), la fonction vérifie chaque transition. S'il y a une incohérence, elle force le passage à un label de début `B-`. Cette étape de \"nettoyage\" permet de reconstruire des segments complets et cohérents, rendant la baseline compatible avec une évaluation stricte."
      ],
      "metadata": {
        "id": "gCbCpA6NsHsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(texts):\n",
        "    \"\"\"\n",
        "    Réalise la prédiction des étiquettes BIO sur un corpus de textes.\n",
        "\n",
        "    Args:\n",
        "        texts (list): Liste de listes de tokens (phrases).\n",
        "\n",
        "    Returns:\n",
        "        list: Liste de listes d'étiquettes prédites.\n",
        "    \"\"\"\n",
        "    all_preds = []\n",
        "    for sentence in texts:\n",
        "        preds = []\n",
        "        prev_tag = \"O\"\n",
        "        for word in sentence:\n",
        "            tag = naive_model.get(word.lower(), \"O\")\n",
        "\n",
        "            # Un label \"I-\" ne peut pas suivre un label \"O\" ou un type différent\n",
        "            if tag.startswith(\"I-\"):\n",
        "                tag_type = tag.split(\"-\")[1]\n",
        "                if prev_tag == \"O\" or tag_type not in prev_tag:\n",
        "                    tag = \"B-\" + tag_type\n",
        "\n",
        "            preds.append(tag)\n",
        "            prev_tag = tag\n",
        "        all_preds.append(preds)\n",
        "    return all_preds"
      ],
      "metadata": {
        "id": "o5rPBe7fo69_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "___\n",
        "##**Evaluation des Performances**\n",
        "\n",
        "Cette étape permet de mesurer l'efficacité de la baseline sur les deux domaines de test via le script d'évaluation fournis.\n",
        "\n",
        "En complément, nous utilisons la bibliothèque `seqeval` pour obtenir une évaluation détaillée, par label, et affiner notre analyse des performances."
      ],
      "metadata": {
        "id": "QqHG6s67or9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_for_script(original_file, preds, output_name):\n",
        "    \"\"\"\n",
        "    Formate et exporte les prédictions au format JSON compatible avec le script d'évaluation (evaluate.py).\n",
        "\n",
        "    Args:\n",
        "        original_file (str): Nom du fichier source dans le répertoire data.\n",
        "        preds (list): Liste des séquences d'étiquettes prédites.\n",
        "        output_name (str): Nom du fichier JSON de sortie à générer.\n",
        "    \"\"\"\n",
        "    path = data_dir / original_file\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    idx = 0\n",
        "    for doc in data:\n",
        "        if 'spans' in doc: del doc['spans']\n",
        "        for paragraph in doc[\"tokens\"]:\n",
        "            if not paragraph: continue\n",
        "            current_sent_preds = preds[idx]\n",
        "            for i, token in enumerate(paragraph):\n",
        "                if i < len(current_sent_preds):\n",
        "                    token[\"arg\"] = current_sent_preds[i]\n",
        "            idx += 1\n",
        "\n",
        "    with open(output_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"\\nFichier '{output_name}' généré.\")\n",
        "\n",
        "export_for_script(\"aae_test.json\", preds_essays, \"naive_essays_final.json\")\n",
        "export_for_script(\"abstrct_neoplasm_test.json\", preds_med, \"naive_med_final.json\")\n",
        "\n",
        "print(\"--- EVALUATION SCRIPT : ESSAIS ---\")\n",
        "!python evaluate.py data/aae_test.json naive_essays_final.json\n",
        "\n",
        "print(\"\\n--- EVALUATION SCRIPT : MÉDICAL ---\")\n",
        "!python evaluate.py data/abstrct_neoplasm_test.json naive_med_final.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6YI-jWzPosUc",
        "outputId": "7fc971dd-a153-4dd3-e288-bd2454d250df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fichier 'naive_essays_final.json' généré.\n",
            "\n",
            "Fichier 'naive_med_final.json' généré.\n",
            "--- EVALUATION SCRIPT : ESSAIS ---\n",
            "\n",
            "\n",
            "********************** SPANS *************************** \n",
            "   STRICT EVALUATION\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 0.006026514373400764 \n",
            "      Recall    : 0.0015998625870651054\n",
            "      F-score   : 0.002487296305431143\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 0.00221530634588596 \n",
            "      Recall    : 0.0006877923492302948 \n",
            "      F-score   : 0.0010359709458027475\n",
            "\n",
            "    RELAXED EVALUATION (α = 0.5)\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 0.7378207072066149 \n",
            "      Recall    : 0.6923472091381503\n",
            "      F-score   : 0.7104404133787858\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 0.6443957102422502 \n",
            "      Recall    : 0.45312841044118274 \n",
            "      F-score   : 0.5277766738653777\n",
            "\n",
            "\n",
            "\n",
            "******************* RELATIONS *************************** \n",
            "   STRICT EVALUATION\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0\n",
            "      F-score   : 1.0\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0 \n",
            "      F-score   : 1.0\n",
            "\n",
            "    RELAXED EVALUATION (α = 0.5)\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0\n",
            "      F-score   : 1.0\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0 \n",
            "      F-score   : 1.0\n",
            "\n",
            "\n",
            "--- EVALUATION SCRIPT : MÉDICAL ---\n",
            "\n",
            "\n",
            "********************** SPANS *************************** \n",
            "   STRICT EVALUATION\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 0.0 \n",
            "      Recall    : 0.0\n",
            "      F-score   : 0.0\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 0.0 \n",
            "      Recall    : 0.0 \n",
            "      F-score   : 0.0\n",
            "\n",
            "    RELAXED EVALUATION (α = 0.5)\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 0.8532238702892393 \n",
            "      Recall    : 0.4768334056085865\n",
            "      F-score   : 0.605783009232619\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 0.7948952774623327 \n",
            "      Recall    : 0.32878862087808586 \n",
            "      F-score   : 0.4588336784337355\n",
            "\n",
            "\n",
            "\n",
            "******************* RELATIONS *************************** \n",
            "   STRICT EVALUATION\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0\n",
            "      F-score   : 1.0\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0 \n",
            "      F-score   : 1.0\n",
            "\n",
            "    RELAXED EVALUATION (α = 0.5)\n",
            "    > Argument mining spans (unlabeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0\n",
            "      F-score   : 1.0\n",
            "    > Argument mining spans (labeled)\n",
            "      Precision : 1.0 \n",
            "      Recall    : 1.0 \n",
            "      F-score   : 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "print(\"\\n--- ESSAIS ---\")\n",
        "preds_essays = predict(test_essays_texts)\n",
        "print(classification_report(test_essays_tags, preds_essays, zero_division=0))\n",
        "\n",
        "print(\"\\n--- MÉDICAL ---\")\n",
        "preds_med = predict(test_med_texts)\n",
        "print(classification_report(test_med_tags, preds_med, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcqgAoOKovzc",
        "outputId": "c1a33f4a-585c-49aa-ac2e-3e093c06670f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ESSAIS ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Claim       0.00      0.00      0.00       301\n",
            "  MajorClaim       0.00      0.00      0.00       151\n",
            "     Premise       0.01      0.09      0.02       799\n",
            "\n",
            "   micro avg       0.01      0.06      0.02      1251\n",
            "   macro avg       0.00      0.03      0.01      1251\n",
            "weighted avg       0.01      0.06      0.01      1251\n",
            "\n",
            "\n",
            "--- MÉDICAL ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Claim       0.00      0.00      0.00        50\n",
            "  MajorClaim       0.00      0.00      0.00         5\n",
            "     Premise       0.00      0.00      0.00        99\n",
            "\n",
            "   micro avg       0.00      0.00      0.00       154\n",
            "   macro avg       0.00      0.00      0.00       154\n",
            "weighted avg       0.00      0.00      0.00       154\n",
            "\n"
          ]
        }
      ]
    }
  ]
}